# VocaGlyph

> A native macOS dictation app that transcribes your voice and pastes the result anywhere â€” instantly, privately, and with optional AI refinement.

VocaGlyph lives in your menu bar. Press a hotkey, speak, release â€” and your words appear in whatever app has focus.

---

## Features

### ğŸ™ï¸ Voice-to-Text Transcription

VocaGlyph supports two transcription backends that can be switched without restarting the app:

| Engine | Description |
|---|---|
| **Apple Native Speech** | Uses macOS 15+ on-device speech recognition. Zero setup, instant results. |
| **WhisperKit (OpenAI Whisper)** | Local CoreML models from `argmaxinc/whisperkit-coreml`. Higher accuracy, larger models. |

**Supported Whisper model variants** include standard OpenAI Whisper sizes (e.g. `large-v3`, `large-v3_turbo`) and `distil-whisper` variants. Models are downloaded and managed directly from the Settings window.

All transcription is done entirely on-device â€” no audio is ever sent to the cloud.

---

### âŒ¨ï¸ Configurable Global Hotkey

Trigger dictation from any app using a system-wide hotkey. Available presets:

- `âŒƒ â‡§ C` (default)
- `âŒ¥ Space`
- `âŒ˜ â‡§ Space`
- `âŒƒ Space`

Hold the hotkey to record, release to transcribe. A debounce guard prevents accidental double-triggers.

---

### ğŸ“‹ Smart Output

After transcription completes, VocaGlyph:

1. **Copies** the result to the system clipboard.
2. **Simulates Cmd+V** via CGEvent to paste directly into the focused app (requires Accessibility permission).
3. **Plays a subtle sound** (`Pop`) to confirm success.

Optionally, **filler word removal** strips common conversational words (`um`, `uh`, `ah`, `like`, `you know`) before pasting.

---

### ğŸŒ Multi-Language Dictation

Choose a primary language in Settings or let Whisper auto-detect from audio:

- Auto-Detect (default)
- English (US)
- Spanish (ES)
- French (FR)
- German (DE)
- Indonesian (ID)

---

### ğŸ¤– AI Post-Processing *(Experimental)*

After transcription, an optional AI pass can fix grammar, punctuation, and capitalization before pasting. Three engine options are available:

#### Apple Intelligence (macOS 26+ / Tahoe)
Uses the on-device Foundation Models framework (~3B parameter model). Fully private, no network required, no cold-start â€” the OS keeps the model loaded.

> **Requirements:** Apple Silicon Mac (M1+), macOS 26 (Tahoe), Apple Intelligence enabled in System Settings.

#### Local AI (MLX / Qwen)
Runs an open-source LLM locally via [mlx-swift-lm](https://github.com/ml-explore/mlx-swift). The default model is `mlx-community/Qwen2.5-7B-Instruct-4bit`. Models are downloaded from HuggingFace and cached in `~/.VocaGlyph/models/`.

- **Background warm-up**: The model is preloaded into Unified Memory on launch so your first dictation has no delay.
- **Memory management**: Free model memory or delete downloaded files directly from Settings.
- Supports any Qwen-style chat model (including thinking models â€” `<think>` blocks are automatically stripped).

#### Cloud API
Sends transcription text to a cloud AI provider for refinement. Supported providers:

| Provider | Model |
|---|---|
| **Google Gemini** | Gemini API |
| **Anthropic Claude** | Claude API |

API keys are stored securely in the **macOS Keychain** â€” never in plaintext or `UserDefaults`.

#### Custom Prompt
All three post-processing engines accept a configurable system prompt so you can tailor the refinement behaviour (e.g. "Fix grammar only", "Translate to formal English", "Return bullet points").

#### Safety Guardrails
Post-processing output is validated before use:
- **Refusal detection** â€” if the model refuses to respond, the raw transcription is used instead.
- **Hallucination gate** â€” output that is 3Ã— longer than the input is discarded.
- **Silence detection** â€” known Whisper phantom phrases (e.g. "Thank you for watching", `[BLANK_AUDIO]`) are silently dropped, producing no output.

---

### ğŸ“œ Transcription History

Every successful dictation is persisted locally using **SwiftData**. The History tab in Settings lets you browse and review past transcriptions. Items older than **30 days** are automatically cleaned up.

---

### ğŸ–¥ï¸ Recording Overlay HUD

A floating overlay panel appears on screen while recording and processing, giving clear visual feedback of the current state:

| State | Menu Bar Icon |
|---|---|
| Idle | App icon |
| Initializing | `gearshape.fill` (yellow) |
| Recording | `waveform.circle.fill` (red) |
| Processing | `hourglass.circle.fill` (orange) |

---

### âš™ï¸ Settings Window

A full-featured native macOS settings window with four tabs:

- **General** â€” hotkey, language, auto-punctuation, filler word removal, launch at login, debug logging.
- **Model** â€” download, switch, and delete Whisper models with per-model download progress.
- **Post-Processing** â€” enable/disable AI refinement, choose engine, manage API keys, set custom prompt.
- **History** â€” browse and search past transcriptions.

---

### ğŸ”’ Privacy & Permissions

VocaGlyph requests only the permissions it needs:

| Permission | Purpose |
|---|---|
| **Microphone** | Capture audio for transcription |
| **Accessibility** | Simulate Cmd+V to paste text |
| **Speech Recognition** (Apple Native) | Use on-device Speech framework |

An onboarding flow walks new users through granting each permission before the app initialises.

---

### ğŸš€ System Integration

- **Menu bar app** â€” no Dock icon, no Cmd+Tab entry.
- **Launch at Login** â€” register/unregister via `SMAppService` directly from Settings with a single toggle.
- **Debug Logging** â€” a structured log file is written to disk when enabled, and can be revealed in Finder from Settings.

---

## Requirements

| Component | Minimum |
|---|---|
| macOS | 15.0 (Sequoia) |
| Apple Silicon | Recommended (required for Apple Intelligence & Local AI) |
| Xcode | 16+ (Xcode 26 for Apple Intelligence engine) |

---

## Architecture Overview

```
VocaGlyph/
â”œâ”€â”€ App/
â”‚   â”œâ”€â”€ AppDelegate.swift        # Menu bar, window setup, service wiring
â”‚   â””â”€â”€ AppStateManager.swift    # Central state machine (idle â†’ recording â†’ processing)
â”œâ”€â”€ Engines/
â”‚   â”œâ”€â”€ Transcription/
â”‚   â”‚   â”œâ”€â”€ NativeSpeechEngine   # macOS Speech framework
â”‚   â”‚   â”œâ”€â”€ WhisperService       # WhisperKit CoreML wrapper
â”‚   â”‚   â””â”€â”€ EngineRouter         # Hot-swappable engine dispatch
â”‚   â””â”€â”€ PostProcessing/
â”‚       â”œâ”€â”€ AppleIntelligenceEngine  # Foundation Models (macOS 26+)
â”‚       â”œâ”€â”€ LocalLLMEngine           # MLX on-device LLM
â”‚       â”œâ”€â”€ GeminiEngine             # Google Gemini API
â”‚       â””â”€â”€ AnthropicEngine          # Anthropic Claude API
â”œâ”€â”€ Services/
â”‚   â”œâ”€â”€ AudioRecorderService     # AVAudioEngine capture
â”‚   â”œâ”€â”€ HotkeyService            # CGEvent global hotkey tap
â”‚   â”œâ”€â”€ OutputService            # Clipboard + CGEvent paste
â”‚   â”œâ”€â”€ KeychainService          # Secure API key storage
â”‚   â”œâ”€â”€ PermissionsService       # Permission checks
â”‚   â””â”€â”€ LoggerService            # Unified file logger
â”œâ”€â”€ UI/
â”‚   â”œâ”€â”€ Settings/                # SwiftUI settings window
â”‚   â””â”€â”€ HUD/                     # Floating recording overlay
â””â”€â”€ Domain/
    â”œâ”€â”€ Protocols.swift          # TranscriptionEngine, PostProcessingEngine
    â””â”€â”€ TranscriptionItem.swift  # SwiftData model for history
```
