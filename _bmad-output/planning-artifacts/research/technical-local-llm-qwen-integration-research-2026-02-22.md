---
stepsCompleted: [1, 2, 3, 4, 5, 6]
inputDocuments:
  - swift-version/Package.swift
  - swift-version/Sources/voice-to-text/Domain/Protocols.swift
  - swift-version/Sources/voice-to-text/Engines/PostProcessing/AppleIntelligenceEngine.swift
  - swift-version/Sources/voice-to-text/Engines/PostProcessing/GeminiEngine.swift
  - swift-version/Sources/voice-to-text/Engines/PostProcessing/AnthropicEngine.swift
workflowType: 'research'
lastStep: 6
research_type: 'technical'
research_topic: 'Adding local LLM (Qwen via MLX-Swift) as a third post-processing option in VocaGlyph'
research_goals: 'Enable on-device, offline, privacy-first text post-processing using Qwen 2.5 models via the existing PostProcessingEngine protocol, without breaking existing Apple Intelligence or external AI (Gemini/Anthropic) options'
user_name: 'Novian'
date: '2026-02-22'
web_research_enabled: true
source_verification: true
---

# Technical Research: Local LLM Post-Processing for VocaGlyph

**Date:** 2026-02-22  
**Author:** Novian  
**Research Type:** Technical  
**Previous Research:** See `technical-llm-integration-research-2026-02-21.md` for broad architectural context.

---

## Executive Summary

The VocaGlyph macOS app currently has two post-processing paths: **Apple Intelligence** (`AppleIntelligenceEngine` â€” currently non-functional due to unavailable public APIs) and **External AI** (`GeminiEngine` / `AnthropicEngine` â€” cloud-dependent, requires API keys, network access). A critical gap exists: **fully-offline, always-available, private text refinement**.

The solution is to add a `LocalLLMEngine` as a fourth conformer to the existing `PostProcessingEngine` protocol, powered by **`mlx-swift-lm` + `MLXLLM`** and targeting the **`mlx-community/Qwen2.5-7B-Instruct-4bit`** model hosted on Hugging Face. This engine runs entirely on-device via Apple Silicon's Unified Memory Architecture, requires no API keys, works offline, and is compatible with 16GB+ Mac configurations.

**Key Findings:**
- **Drop-in protocol conformance:** The `PostProcessingEngine` protocol (`refine(text:prompt:) async throws -> String`) requires zero changes to add a new engine.
- **Recommended framework:** `mlx-swift-lm` (SPM: `https://github.com/ml-explore/mlx-swift-lm`) provides `MLXLLM` + `MLXLMCommon` targets â€” the highest-level, most maintainable Swift-native path.
- **Recommended model:** `mlx-community/Qwen2.5-7B-Instruct-4bit` â€” ~4.3 GB on disk, runs comfortably on 16GB unified memory Macs, excellent instruction-following quality for text refinement tasks.
- **Smaller alternative:** `mlx-community/Qwen2.5-1.5B-Instruct-4bit` â€” ~1.1 GB, suitable for 8GB Macs with reduced quality.
- **Model delivery:** Models are downloaded at first use from Hugging Face Hub using the `Hub` package (already a transitive dependency via WhisperKit's ecosystem). Stored in `~/Library/Application Support/VocaGlyph/LocalModels/`.
- **Memory management:** Model must be lazy-loaded and explicitly evicted after each use to avoid sustained memory pressure on the Whisper transcription pipeline.

---

## Technical Research Scope Confirmation

**Research Topic:** Adding local LLM (Qwen via MLX-Swift) as a third post-processing option in VocaGlyph  
**Research Goals:** Enable on-device, offline, privacy-first text post-processing using Qwen 2.5 models via the existing `PostProcessingEngine` protocol, without breaking existing Apple Intelligence or external AI options

**Technical Research Scope:**
- Architecture Analysis â€” how the new engine fits the existing `PostProcessingEngine` protocol
- Implementation Approaches â€” MLX-Swift vs Llama.cpp, model loading, memory lifecycle
- Technology Stack â€” SPM packages, HuggingFace Hub, Qwen model variants
- Integration Patterns â€” `Swift actor`, lazy-loading, memory eviction, Settings UI changes
- Performance Considerations â€” TTFT (Time to First Token), memory pressure, quantization trade-offs

**Scope Confirmed:** 2026-02-22

---

## 1. Current Architecture Analysis

### Existing Post-Processing System

The current system has an elegant, extensible design:

```swift
// Domain/Protocols.swift
public protocol PostProcessingEngine: Sendable {
    func refine(text: String, prompt: String) async throws -> String
}
```

**Existing conformers:**

| Engine | Type | Status | Requires |
|---|---|---|---|
| `AppleIntelligenceEngine` | Local (System) | âš ï¸ Non-functional (API unavailable) | macOS 15.1+ |
| `GeminiEngine` | Cloud | âœ… Working | Gemini API key, internet |
| `AnthropicEngine` | Cloud | âœ… Working | Anthropic API key, internet |
| **`LocalLLMEngine`** | **Local (MLX)** | **ðŸ†• To be added** | **16GB+ Mac, ~4.3GB disk** |

The new `LocalLLMEngine` will be the **only fully functional local engine** that works without any API keys or internet connection.

### Package.swift (Current State)

```swift
dependencies: [
    .package(url: "https://github.com/argmaxinc/WhisperKit", from: "0.10.0")
]
```

One new dependency must be added. No existing dependencies are modified.

---

## 2. Technology Stack Analysis

### Framework Comparison: MLX-Swift vs Llama.cpp

| Criterion | `mlx-swift-lm` (MLXLLM) | `llama.cpp` via `LLM.swift` |
|---|---|---|
| Native Apple Silicon | âœ… Native UMA, no data copying | âš ï¸ Metal backend, some overhead |
| Swift API quality | âœ… First-class `async/await`, `AsyncSequence` | âš ï¸ C++ wrapper, thinner Swift layer |
| SPM integration | âœ… Native SPM package | âš ï¸ Complex binary target or SPM wrapper |
| Model format | `.safetensors` (MLX format) | `.gguf` |
| Maintained by Apple | âœ… Yes | â›” No â€” community maintained |
| Model availability | âœ… `mlx-community` on HuggingFace | âœ… GGUF models on HuggingFace |
| Streaming support | âœ… `AsyncSequence` tokens | âš ï¸ Callback-based |
| **Recommendation** | **âœ… PREFERRED** | âš ï¸ Viable alternative |

**Decision:** Use `mlx-swift-lm` (`MLXLLM`) as the primary framework. It is the Apple-endorsed, native-Swift, most performant option for Apple Silicon.

_Source: [mlx-swift-lm on GitHub](https://github.com/ml-explore/mlx-swift-lm), [Apple MLX Swift Blog](https://swift.org/blog/mlx-swift/), [Apple LLMEval Example](https://github.com/ml-explore/mlx-swift-examples)_

### Recommended Model: Qwen2.5-7B-Instruct (4-bit)

| Model ID | Size | Min RAM | Quality |
|---|---|---|---|
| `mlx-community/Qwen2.5-0.5B-Instruct-4bit` | ~0.4 GB | 8 GB | Basic â€” suitable for simple corrections |
| `mlx-community/Qwen2.5-1.5B-Instruct-4bit` | ~1.1 GB | 8 GB | Good â€” reasonable instruction following |
| **`mlx-community/Qwen2.5-7B-Instruct-4bit`** | **~4.3 GB** | **16 GB** | **Strong â€” recommended default** |
| `mlx-community/Qwen2.5-14B-Instruct-4bit` | ~8.5 GB | 24 GB | Excellent â€” for power users |

**Recommended default:** `mlx-community/Qwen2.5-7B-Instruct-4bit`  
**Fallback for 8GB Macs:** `mlx-community/Qwen2.5-1.5B-Instruct-4bit`

The 7B 4-bit model achieves strong instruction-following quality for grammar correction, rephrasing, and text cleanup â€” the primary post-processing use cases in VocaGlyph.

_Source: [HuggingFace mlx-community/Qwen2.5-7B-Instruct-4bit](https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit)_

### SPM Integration

```swift
// Package.swift addition
.package(url: "https://github.com/ml-explore/mlx-swift-lm", branch: "main")

// Target dependencies
.product(name: "MLXLLM", package: "mlx-swift-lm"),
.product(name: "MLXLMCommon", package: "mlx-swift-lm"),
```

> [!NOTE]
> `mlx-swift-lm` transitively depends on `mlx-swift`, `Jinja`, and `Hub`. The `Hub` package is already a well-known transitive dependency in the MLX ecosystem and handles Hugging Face model caching.

---

## 3. Integration Pattern: `LocalLLMEngine`

### Actor-Based Engine Design

Following the existing pattern (all engines are `actor` types), the new engine must be:

```swift
// Engines/PostProcessing/LocalLLMEngine.swift
import MLXLLM
import MLXLMCommon

public actor LocalLLMEngine: PostProcessingEngine {
    
    private var loadedModel: ModelContainer?
    private let modelConfiguration: ModelConfiguration
    
    public init(modelId: String = "mlx-community/Qwen2.5-7B-Instruct-4bit") {
        self.modelConfiguration = ModelRegistry.configuration(id: modelId) 
            ?? ModelConfiguration(id: modelId)
    }
    
    public func refine(text: String, prompt: String) async throws -> String {
        let container = try await ensureModelLoaded()
        let fullPrompt = buildPrompt(system: prompt, userText: text)
        return try await container.perform { model, tokenizer in
            // Generate response using MLX token generation
            let result = try await generate(
                model: model,
                tokenizer: tokenizer,
                prompt: fullPrompt,
                parameters: GenerateParameters(temperature: 0.2, maxTokens: 1024)
            )
            return result.trimmingCharacters(in: .whitespacesAndNewlines)
        }
    }
    
    // MARK: - Lifecycle
    
    public func unloadModel() {
        loadedModel = nil  // Releases all MLX memory
        Logger.shared.info("LocalLLMEngine: Model unloaded from memory.")
    }
    
    private func ensureModelLoaded() async throws -> ModelContainer {
        if let existing = loadedModel { return existing }
        Logger.shared.info("LocalLLMEngine: Loading model '\(modelConfiguration.id)'...")
        let container = try await loadModelContainer(configuration: modelConfiguration)
        loadedModel = container
        Logger.shared.info("LocalLLMEngine: Model loaded successfully.")
        return container
    }
    
    private func buildPrompt(system: String, userText: String) -> String {
        // Chat template format for Qwen2.5 Instruct models
        return "<|im_start|>system\n\(system)<|im_end|>\n<|im_start|>user\n\(userText)<|im_end|>\n<|im_start|>assistant\n"
    }
}

// MARK: - Errors
public enum LocalLLMEngineError: LocalizedError {
    case modelLoadFailed(String)
    case inferenceFailed(String)
    case insufficientMemory
    
    public var errorDescription: String? {
        switch self {
        case .modelLoadFailed(let reason):
            return "Failed to load local AI model: \(reason)"
        case .inferenceFailed(let reason):
            return "Local AI inference failed: \(reason)"
        case .insufficientMemory:
            return "Insufficient memory to load the local AI model. Please select a smaller model in Settings."
        }
    }
}
```

> [!IMPORTANT]
> The `unloadModel()` method is critical. The Whisper transcription pipeline and LLM inference **both** consume significant unified memory. The `LocalLLMEngine` must actively evict its model weights after each transcription session ends to avoid memory pressure on Whisper.

### Memory Lifecycle Pattern

```
User triggers recording:
  â””â”€ No action needed (LLM idle)

User stops recording:
  â””â”€ Whisper transcribes audio
  â””â”€ AppStateManager calls postProcessor.refine(text:prompt:)
      â””â”€ LocalLLMEngine.ensureModelLoaded()  [first time: ~5-10s download/load]
      â””â”€ Runs inference  (~1-3s for short texts on M2)
      â””â”€ Returns refined text

After N minutes idle (configurable, default: 5min):
  â””â”€ AppStateManager calls localLLMEngine.unloadModel()
  â””â”€ MLX memory freed

User switches away from Local LLM in Settings:
  â””â”€ AppStateManager calls localLLMEngine.unloadModel()
```

### Settings UI Changes Required

The Settings UI needs a new "Local AI Model" option in the post-processing engine selector. Additionally, a sub-section should appear when Local AI is selected:

**New Settings controls to add:**
1. **Engine selector:** Add `Local AI (Qwen)` as a new `.localLLM` case in the `PostProcessingProvider` enum (or equivalent settings key)
2. **Model selector:** Picker with `[Qwen 2.5 0.5B (0.4GB), Qwen 2.5 1.5B (1.1GB), Qwen 2.5 7B (4.3GB â€” Default)]`
3. **Download status:** Show download progress / "Downloaded" / "Not downloaded" state
4. **Memory warning:** Show a notice: "Requires ~4.3GB disk space and 16GB+ RAM. Model is downloaded once and cached."
5. **Unload button:** "Free memory now" button that calls `localLLMEngine.unloadModel()`

---

## 4. Architectural Patterns

### Engine Selection Flow (AppStateManager / AppDelegate)

The engine routing currently happens in `AppStateManager`. The pattern for the `localLLM` case:

```swift
// Pseudo-code for engine selection
func makePostProcessingEngine(for provider: PostProcessingProvider) -> any PostProcessingEngine {
    switch provider {
    case .appleIntelligence: return AppleIntelligenceEngine()
    case .gemini:            return GeminiEngine(keychainService: keychainService)
    case .anthropic:         return AnthropicEngine(keychainService: keychainService)
    case .localLLM:          return localLLMEngineInstance  // Singleton â€” model must persist between calls
    }
}
```

> [!IMPORTANT]
> **`LocalLLMEngine` must be a singleton** (one instance held by `AppStateManager`), unlike the other engines. Re-creating it on each call would trigger expensive model reloads. The engine itself handles lazy loading internally.

### Model Download Strategy

Models are downloaded via the `Hub` package (part of the MLX Swift ecosystem) on first use:

1. On first `refine()` call â†’ `loadModelContainer()` checks local cache in `~/Library/Caches/huggingface/`
2. If not cached â†’ downloads model shards (~4.3GB) with progress callback
3. On subsequent calls â†’ loads from cache (fast, <3s)

The Settings UI should allow pre-downloading the model before first use via a "Download Model" button.

---

## 5. Performance Considerations

### Expected Performance on Apple Silicon

| Mac Config | Model | TTFT (est.) | Throughput |
|---|---|---|---|
| M1 8GB | Qwen2.5-1.5B-4bit | ~1s | ~25 tok/s |
| M2 16GB | Qwen2.5-7B-4bit | ~2-3s | ~18-22 tok/s |
| M3 Pro 18GB | Qwen2.5-7B-4bit | ~1-2s | ~25-30 tok/s |
| M3 Max 48GB | Qwen2.5-14B-4bit | ~2s | ~20-25 tok/s |

For post-processing short transcription texts (typically 10-200 words), total generation time is **1-5 seconds** â€” acceptable for an async background operation.

_Source: Community benchmarks from [mlx-community](https://github.com/ml-explore/mlx-swift-examples), [dev.to MLX benchmarks](https://dev.to/), [Apple WWDC 2025 MLX Labs](https://developer.apple.com/wwdc25/)_

### First-Load Penalty

The **first call** after model download carries a significant penalty:
- **Model download:** ~4.3GB, varies by internet speed (one-time only)
- **Model cold load:** ~5-10 seconds on M2 (subsequent loads from disk: ~2-3s)
- **Inference:** 1-5s for typical transcription texts

**Mitigation:** Show a loading indicator in the HUD during model load, with the message "Loading local AI model... (one-time setup)".

---

## 6. Implementation Recommendations

### Priority Order

1. **Add `mlx-swift-lm` to Package.swift** â€” single dependency addition
2. **Create `LocalLLMEngine.swift`** â€” new engine conforming to `PostProcessingEngine`
3. **Add `PostProcessingProvider.localLLM` enum case** â€” settings value
4. **Update `AppStateManager`** â€” singleton instance, engine routing
5. **Update `SettingsView`** â€” model picker, download status UI, memory warning
6. **Implement auto-eviction** â€” unload model after idle timeout (e.g., 5 minutes)

### Risk Mitigation

| Risk | Mitigation |
|---|---|
| Memory pressure crashes | Strict `unloadModel()` after idle, recommend 16GB+ |
| Long first-load time | Progress UI in HUD, pre-download in Settings |
| Model download failure | Graceful error with retry, no crash |
| `mlx-swift-lm` API changes | Pin to a tagged release rather than `branch: "main"` in production |
| 8GB Mac compatibility | Allow 1.5B model option with quality warning |

---

## 7. Technical Research Conclusion

Adding a local LLM post-processing engine to VocaGlyph is **technically well-scoped and architecturally clean**:

- **Zero protocol changes** â€” `PostProcessingEngine` already supports it.
- **One new file** â€” `LocalLLMEngine.swift` in the existing `Engines/PostProcessing/` directory.
- **One SPM dependency** â€” `mlx-swift-lm`.
- **Minimal Settings changes** â€” new picker option and download UI.

The `mlx-community/Qwen2.5-7B-Instruct-4bit` model is the ideal default: strong quality, reasonable size (~4.3GB), excellent instruction following for text refinement, and officially supported by the Apple MLX community.

The main engineering challenges are **memory lifecycle management** (ensuring the LLM and Whisper don't compete for unified memory) and **first-load UX** (managing the user experience during model download/cold-load). Both are well-understood problems with established patterns.

---

**Technical Research Completion Date:** 2026-02-22  
**Research Period:** Current comprehensive technical analysis  
**Source Verification:** Verified against Apple mlx-swift-lm GitHub, HuggingFace mlx-community, and Apple developer documentation.  
**Technical Confidence Level:** High â€” MLX-Swift + Qwen2.5 is a production-validated pattern with Apple reference examples (LLMEval).

_Sources: [mlx-swift-lm](https://github.com/ml-explore/mlx-swift-lm) Â· [Qwen2.5-7B-Instruct-4bit on HuggingFace](https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit) Â· [Apple LLMEval Example](https://github.com/ml-explore/mlx-swift-examples) Â· [Apple MLX Blog on swift.org](https://swift.org/blog/mlx-swift/) Â· [WWDC 2025 MLX Labs](https://developer.apple.com/wwdc25/)_
